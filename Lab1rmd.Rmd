---
title: "Practical 1"
author: "Blanco Romain, Carlin Cyril, Dong St√©phane"
date: "02/26/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Exercise 1

### Question 1

```{r}
# Define the parameters
n <- 6000
p <- 201
# Set the seed to 0
set.seed(0)
# Create the matrix for the Question 1
X <- matrix(nrow = n, ncol = p)
for (i in 1:n) {
  X[i,] = matrix(rnorm(p, mean = 0, sd = 1), nrow = 1, ncol = p)
}
# Store it into a data frame
D <- data.frame(variables=X)
```

### Question 2

We use the matrix X defined in the previous question.\
Let $y$ be the 1st variable : $$ y = \begin{pmatrix} x_{1,1} \\
\vdots \\
x_{n,1}\end{pmatrix}$$\
We define, 
$$ MatX = \begin{pmatrix} 1 & x_{1,2} & \cdots & x_{1,p} \\ 
1 & x_{2,2} & \cdots & x_{2,p} \\
\vdots & \vdots & \ddots & \vdots \\
1 & x_{n,2} & \cdots & x_{n,p} \end{pmatrix}$$\

We obtain : $\hat\beta = (MatX^{T}\ MatX)^{-1}\ MatX^{T}\ y$.\
The linear model using the last 200 variables to predict the first one is : $y = X\hat\beta + \epsilon$, with $\epsilon = (\epsilon_1, ..., \epsilon_n)$.\
\
The true regression model is : $y = f(X) + \epsilon$.\
The difference is that for the true regression model, f can be a non-linear function depending on the data.\
The linear regression model can be a possibility for the true regression model.

### Question 3

We use the following code to get the coefficients ($\hat\beta$) : \
```{r}
# Define the parameters
n <- 6000
p <- 201
# Set the seed to 0
set.seed(0)
# Create the matrix for the Question 1
X <- matrix(nrow = n, ncol = p)
for (i in 1:n) {
  X[i,] = matrix(rnorm(p, mean = 0, sd = 1), nrow = 1, ncol = p)
}
# Store it into a data frame
D <- data.frame(variables=X)
# Create the design matrix
MatX <- cbind(1, X[,2:p])
y <- D$variables.1
# Simulate the linear regression model, get the betas and the epsilons
fit <- lm(y ~ MatX)
beta <- summary(fit)$coef[,"Estimate"]
epsilon <- fit$residuals
```
The linear model is thus : $$ y = MatX \times \hat\beta + \hat\epsilon$$ with $\hat\beta$ and $\hat\epsilon$ calculated above (called "beta" and "epsilon").\
\
Then, we compute the number of coefficients assessed as significantly non-zero at level 5% with the following code :\
```{r}
# Compute the number of coefficients assessed as significantly non-zero at level 5%
level <- 0.05
P_values <- summary(fit)$coef[,"Pr(>|t|)"]
counter <- 0
for (i in P_values) {
  if (i < level)
    counter = counter + 1
}
sprintf("Number of coefficients assessed as significantly non-zero at level %d %% : %d out of %d", level * 100, counter, p)
```
We observe that only around 5.5% (11 out of 201) of the coefficients can be considered as non-zero at level 5%.\
That result is not very surprising because all of the variables are independant. The first variable should not be dependant on (almost) any of the other 200 variables. There are still some variables (close to 5% of them) that are assessed as significantly non-zero at level 5% but that is quite normal due to the 5% tolerance level that we have set.

## Exercise 2

**2**)



**b**)
if we take the formula of both predictors in replacing, we have :
$$\begin{aligned}
Y_{i} = 2 + 4X_{1,i} + \epsilon_{2,i} + \epsilon_{3,i} \\
      =  2 + X_{2,i} + \epsilon_{1,i} + \epsilon_{3,i}
\end{aligned}$$


In theory, we should have for the first predictor $\beta_{0} = 2$ and $\beta_{1} = 4$ then for the second predictor $\beta_{0} = 2$ and $\beta_{2} = 1$. We realize the linear regression in R. 
```{r}
set.seed(3)
n=10
X1=rnorm(n)
X2=3*X1+rnorm(n)
Y=2+X1+X2+rnorm(n)
predictor1 <- lm(Y~X1)
predictor2 <- lm(Y~X2)
b01=predictor1$coefficients[1]
b1=predictor1$coefficients[2]
b02=predictor2$coefficients[1]
b2=predictor2$coefficients[2]
summary(predictor1)
summary(predictor2)
list(beta0=b01,beta1=b1,beta0=b02,beta1=b2)


```


First of all, the two models have a good R-square so that seems to be good models.
But, in the first model, the $\beta_{0}$ is enough far of the theory value and in the second model, it's the $\beta_{2}$.
So, the predictors are not really good that's perhaps due to the sum of the different residuals terms. 

**c**) 


```{r}
set.seed(3)
n=10
X1=rnorm(n)
X2=3*X1+rnorm(n)
Y=2+X1+X2+rnorm(n)
predictor <- lm(Y~X1+X2)
summary(predictor)
```


There are two points of views : 
-According to the F-test the both predictors X1 and X2 have effects on Y because the p-value is small.
-But according to the two T-tests, X1 has no effect and X2 too. Because, it not differentiates the two because of the sum perhaps.


**e**)


```{r}
set.seed(3)
n=10
X1=rnorm(n)
X2=3*X1+rnorm(n)
Y=2+X1+X2+rnorm(n)
m1 = mean(X1)
m2 = mean(X2)
m = c(m1,m2)
C <- matrix(nrow = 2, ncol = 2)
C[1,1]=var(X1)
C[2,2]=var(X2)
C[1,2]=cov(X1,X2)
C[2,1]=C[1,2]

ellipses = function(m, CV, probs)
{
  # Compute and plot an ellipse region for bivariate Gaussians, i.e., some ellipse that 
  # has probability probs to contain a 2D Gaussian vector with given parameters.
  # Inputs: 
  #   - m: means
  #   - CV: covariance matrix
  #   - probs: probabilities
  # Source : https://waterprogramming.wordpress.com/2016/11/07/plotting-probability-ellipses-for-bivariate-normal-distributions/
  
  # Coordinates of mean
  b1 = m[1]
  b2 = m[2]
  
  eg = eigen(CV)
  Evec = eg$vectors
  Eval = eg$values
  
  theta = seq(0,2*pi,0.01) # angles used for plotting ellipses
  
  vec.norm = function(v) { sqrt(t(v) %*% v)}
  # compute angle for rotation of ellipse
  # rotation angle will be angle between x axis and first eigenvector
  x.vec = c(1, 0) # vector along x-axis
  cosrotation = t(x.vec) %*% Evec[,1]/(vec.norm(x.vec)*vec.norm(Evec[,1]))
  rotation = pi/2-acos(cosrotation) # rotation angle
  #create a rotation matrix
  R  = matrix(c(sin(rotation), cos(rotation), -cos(rotation), sin(rotation)), 
              nrow=2, ncol=2, byrow = TRUE)
  
  # Create chi squared vector
  chisq = qchisq(probs,2) # percentiles of chi^2 dist df=2
  
  # size ellipses for each quantile
  xRadius = rep(0, length(chisq))
  yRadius = rep(0, length(chisq))
  x = list()
  y = list()
  x.plot = list()
  y.plot = list()
  rotated_Coords = list()
  for (i in 1:length(chisq)) {
    # calculate the radius of the ellipse
    xRadius[i]=(chisq[i]*Eval[1])^.5; # primary axis
    yRadius[i]=(chisq[i]*Eval[2])^.5; # secondary axis
    # lines for plotting ellipse
    x[[i]] = xRadius[i]* cos(theta);
    y[[i]] = yRadius[i] * sin(theta);
    # rotate ellipse
    rotated_Coords[[i]] = R %*% matrix(c(x[[i]], y[[i]]), nrow=2, byrow=TRUE)
    # center ellipse
    x.plot[[i]] = t(rotated_Coords[[i]][1,]) + b1
    y.plot[[i]] = t(rotated_Coords[[i]][2,]) + b2}
  
  xlim = range(x.plot[[i]])
  ylim = range(y.plot[[i]])
  plot(b1,b2, xlab = "X1", ylab = "X2", xlim = xlim, ylim = ylim, cex=1)
  abline(h=0)
  abline(v=0)
  # Plot contours
  for (j in 1:length(chisq)) {
    points(x.plot[[j]],y.plot[[j]], cex=0.1)}
  
  legend("bottomright", c('Ellipse region', paste(probs)))}



ellipses(m,C,c(0.5,0.9,0.999,1-2.92e-05))

```





