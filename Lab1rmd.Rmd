---
title: "Practical 1"
author: "Blanco Romain, Carlin Cyril, Dong St√©phane"
date: "02/26/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Exercise 1

### Question 1

```{r}
# Define the parameters
n <- 6000
p <- 201
# Set the seed to 0
set.seed(0)
# Create the matrix for the Question 1
X <- matrix(nrow = n, ncol = p)
for (i in 1:n) {
  X[i,] = matrix(rnorm(p, mean = 0, sd = 1), nrow = 1, ncol = p)
}
# Store it into a data frame
D <- data.frame(variables=X)
```

### Question 2

We use the matrix X defined in the previous question.\
Let $y$ be the 1st variable : $$ y = \begin{pmatrix} x_{1,1} \\
\vdots \\
x_{n,1}\end{pmatrix}$$\
We define, 
$$ MatX = \begin{pmatrix} 1 & x_{1,2} & \cdots & x_{1,p} \\ 
1 & x_{2,2} & \cdots & x_{2,p} \\
\vdots & \vdots & \ddots & \vdots \\
1 & x_{n,2} & \cdots & x_{n,p} \end{pmatrix}$$\

We obtain : $\hat\beta = (MatX^{T}\ MatX)^{-1}\ MatX^{T}\ y$.\
The linear model using the last 200 variables to predict the first one is : $y = X\hat\beta + \epsilon$, with $\epsilon = (\epsilon_1, ..., \epsilon_n)$.\
\
The true regression model is : $y = f(X) + \epsilon$.\
The difference is that for the true regression model, f can be a non-linear function depending on the data.\
The linear regression model can be a possibility for the true regression model.

### Question 3

We use the following code to get the coefficients ($\hat\beta$) : \
```{r}
# Define the parameters
n <- 6000
p <- 201
# Set the seed to 0
set.seed(0)
# Create the matrix for the Question 1
X <- matrix(nrow = n, ncol = p)
for (i in 1:n) {
  X[i,] = matrix(rnorm(p, mean = 0, sd = 1), nrow = 1, ncol = p)
}
# Store it into a data frame
D <- data.frame(variables=X)
# Create the design matrix
MatX <- cbind(1, X[,2:p])
y <- D$variables.1
# Simulate the linear regression model, get the betas and the epsilons
fit <- lm(y ~ MatX)
beta <- summary(fit)$coef[,"Estimate"]
epsilon <- fit$residuals
```
The linear model is thus : $$ y = MatX \times \hat\beta + \hat\epsilon$$ with $\hat\beta$ and $\hat\epsilon$ calculated above (called "beta" and "epsilon").\
\
Then, we compute the number of coefficients assessed as significantly non-zero at level 5% with the following code :\
```{r}
# Compute the number of coefficients assessed as significantly non-zero at level 5%
level <- 0.05
P_values <- summary(fit)$coef[,"Pr(>|t|)"]
counter <- 0
for (i in P_values) {
  if (i < level)
    counter = counter + 1
}
sprintf("Number of coefficients assessed as significantly non-zero at level %d %% : %d out of %d", level * 100, counter, p)
```
We observe that only around 5.5% (11 out of 201) of the coefficients can be considered as non-zero at level 5%.\
That result is not very surprising because all of the variables are independant. The first variable should not be dependant on (almost) any of the other 200 variables. There are still some variables (close to 5% of them) that are assessed as significantly non-zero at level 5% but that is quite normal due to the 5% tolerance level that we have set.

## Exercise 2

**2**)



**b**)
if we take the formula of both predictors in replacing, we have :
$$\begin{aligned}
Y_{i} = 2 + 4X_{1,i} + \epsilon_{2,i} + \epsilon_{3,i} \\
      =  2 + X_{2,i} + \epsilon_{1,i} + \epsilon_{3,i}
\end{aligned}$$


In theory, we should have for the first predictor $\beta_{0} = 2$ and $\beta_{1} = 4$ then for the second predictor $\beta_{0} = 2$ and $\beta_{2} = 1$. We realize the linear regression in R. 
```{r}
set.seed(3)
n=10
X1=rnorm(n)
X2=3*X1+rnorm(n)
Y=2+X1+X2+rnorm(n)
predictor1 <- lm(Y~X1)
predictor2 <- lm(Y~X2)
b01=predictor1$coefficients[1]
b1=predictor1$coefficients[2]
b02=predictor2$coefficients[1]
b2=predictor2$coefficients[2]
summary(predictor1)
summary(predictor2)
list(beta0=b01,beta1=b1,beta0=b02,beta1=b2)


```


First of all, the two models have a good R-square so that seems to be good models.
But, in the first model, the $\beta_{0}$ is enough far of the theory value and in the second model, it's the $\beta_{2}$.
So, the predictors are not really good that's perhaps due to the sum of the different residuals terms. 


