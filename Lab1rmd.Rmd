---
title: "Practical 1"
author: "Blanco Romain, Carlin Cyril, Dong St√©phane"
date: "02/26/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Exercise 1

### Question 1

```{r}
set.seed(0)

A <- matrix(nrow = 6000, ncol = 201)
for (i in 1:6000) {
  A[i,] = matrix(rnorm(201, mean = 0, sd = 1), nrow = 1, ncol = 201)
}

dataFrame <- data.frame(A)
```

### Question 2

Let $y$ be the 1st variable : $$ y = \begin{pmatrix} a_{1,1} \\
\vdots \\
a_{n, 1}\end{pmatrix}$$\
We define, 
$$ X = \begin{pmatrix} 1 & a_{1,2} & \cdots & a_{1,p} \\ 
1 & a_{2,2} & \cdots & a_{2,p} \\
\vdots & \vdots & \ddots & \vdots \\
1 & a_{n,2} & \cdots & a_{n,p} \end{pmatrix}$$\

We obtain : $\hat{\beta} = (X^{T}X)^{-1}X^{T}y$.\
The regression model is : $Y = X\hat{\beta} + \epsilon$, with $\epsilon = (\epsilon_1, ..., \epsilon_n)$.\
\
The true regression model is : $Y = X\hat{\beta}$.\
It is realized only if the output is exactly a linear function of the dependant variables (no residual).

### Question 3

```{r}

y <- rnorm(6000, mean = 0, sd = 1)
beta <- solve(t(X) %*% X) %*% t(X) %*% y
xbeta <- X %*% beta
reg <- lm(y~xbeta)
summary(reg)
```

## Exercise 2

**2**)



**b**)
if we take the formula of both predictors in replacing, we have :
$$\begin{aligned}
Y_{i} = 2 + 4X_{1,i} + \epsilon_{2,i} + \epsilon_{3,i} \\
      =  2 + X_{2,i} + \epsilon_{1,i} + \epsilon_{3,i}
\end{aligned}$$


In theory, we should have for the first predictor $\beta_{0} = 2$ and $\beta_{1} = 4$ then for the second predictor $\beta_{0} = 2$ and $\beta_{2} = 1$. We realize the linear regression in R. 
```{r}
set.seed(3)
n=10
X1=rnorm(n)
X2=3*X1+rnorm(n)
Y=2+X1+X2+rnorm(n)
predictor1 <- lm(Y~X1)
predictor2 <- lm(Y~X2)
b01=predictor1$coefficients[1]
b1=predictor1$coefficients[2]
b02=predictor2$coefficients[1]
b2=predictor2$coefficients[2]
summary(predictor1)
summary(predictor2)
list(beta0=b01,beta1=b1,beta0=b02,beta1=b2)


```


First of all, the two models have a good R-square so that seems to be good models.
But, in the first model, the $\beta_{0}$ is enough far of the theory value and in the second model, it's the $\beta_{2}$.
So, the predictors are not really good that's perhaps due to the sum of the different residuals terms. 

**c**) 


```{r}
set.seed(3)
n=10
X1=rnorm(n)
X2=3*X1+rnorm(n)
Y=2+X1+X2+rnorm(n)
predictor <- lm(Y~X1+X2)
summary(predictor)
```


There are two points of views : 
-According to the F-test the both predictors X1 and X2 have effects on Y because the p-value is small.
-But according to the two T-tests, X1 has no effect and X2 too. Because, it not differentiates the two because of the sum perhaps.


**e**)


```{r}
set.seed(3)
n=10
X1=rnorm(n)
X2=3*X1+rnorm(n)
Y=2+X1+X2+rnorm(n)
m1 = mean(X1)
m2 = mean(X2)
m = c(m1,m2)
C <- matrix(nrow = 2, ncol = 2)
C[1,1]=var(X1)
C[2,2]=var(X2)
C[1,2]=cov(X1,X2)
C[2,1]=C[1,2]

ellipses = function(m, CV, probs)
{
  # Compute and plot an ellipse region for bivariate Gaussians, i.e., some ellipse that 
  # has probability probs to contain a 2D Gaussian vector with given parameters.
  # Inputs: 
  #   - m: means
  #   - CV: covariance matrix
  #   - probs: probabilities
  # Source : https://waterprogramming.wordpress.com/2016/11/07/plotting-probability-ellipses-for-bivariate-normal-distributions/
  
  # Coordinates of mean
  b1 = m[1]
  b2 = m[2]
  
  eg = eigen(CV)
  Evec = eg$vectors
  Eval = eg$values
  
  theta = seq(0,2*pi,0.01) # angles used for plotting ellipses
  
  vec.norm = function(v) { sqrt(t(v) %*% v)}
  # compute angle for rotation of ellipse
  # rotation angle will be angle between x axis and first eigenvector
  x.vec = c(1, 0) # vector along x-axis
  cosrotation = t(x.vec) %*% Evec[,1]/(vec.norm(x.vec)*vec.norm(Evec[,1]))
  rotation = pi/2-acos(cosrotation) # rotation angle
  #create a rotation matrix
  R  = matrix(c(sin(rotation), cos(rotation), -cos(rotation), sin(rotation)), 
              nrow=2, ncol=2, byrow = TRUE)
  
  # Create chi squared vector
  chisq = qchisq(probs,2) # percentiles of chi^2 dist df=2
  
  # size ellipses for each quantile
  xRadius = rep(0, length(chisq))
  yRadius = rep(0, length(chisq))
  x = list()
  y = list()
  x.plot = list()
  y.plot = list()
  rotated_Coords = list()
  for (i in 1:length(chisq)) {
    # calculate the radius of the ellipse
    xRadius[i]=(chisq[i]*Eval[1])^.5; # primary axis
    yRadius[i]=(chisq[i]*Eval[2])^.5; # secondary axis
    # lines for plotting ellipse
    x[[i]] = xRadius[i]* cos(theta);
    y[[i]] = yRadius[i] * sin(theta);
    # rotate ellipse
    rotated_Coords[[i]] = R %*% matrix(c(x[[i]], y[[i]]), nrow=2, byrow=TRUE)
    # center ellipse
    x.plot[[i]] = t(rotated_Coords[[i]][1,]) + b1
    y.plot[[i]] = t(rotated_Coords[[i]][2,]) + b2}
  
  xlim = range(x.plot[[i]])
  ylim = range(y.plot[[i]])
  plot(b1,b2, xlab = "X1", ylab = "X2", xlim = xlim, ylim = ylim, cex=1)
  abline(h=0)
  abline(v=0)
  # Plot contours
  for (j in 1:length(chisq)) {
    points(x.plot[[j]],y.plot[[j]], cex=0.1)}
  
  legend("bottomright", c('Ellipse region', paste(probs)))}



ellipses(m,C,c(0.5,0.9,0.999,1-2.92e-05))

```





