---
title: "Practical 1"
author: "Blanco Romain, Carlin Cyril, Dong St√©phane"
date: "02/26/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Exercise 1

### Question 1

```{r}
set.seed(0)

A <- matrix(nrow = 6000, ncol = 201)
for (i in 1:6000) {
  A[i,] = matrix(rnorm(201, mean = 0, sd = 1), nrow = 1, ncol = 201)
}

dataFrame <- data.frame(A)
```

### Question 2

Let $y$ be the 1st variable : $$ y = \begin{pmatrix} a_{1,1} \\
\vdots \\
a_{n, 1}\end{pmatrix}$$\
We define, 
$$ X = \begin{pmatrix} 1 & a_{1,2} & \cdots & a_{1,p} \\ 
1 & a_{2,2} & \cdots & a_{2,p} \\
\vdots & \vdots & \ddots & \vdots \\
1 & a_{n,2} & \cdots & a_{n,p} \end{pmatrix}$$\

We obtain : $\hat{\beta} = (X^{T}X)^{-1}X^{T}y$.\
The regression model is : $Y = X\hat{\beta} + \epsilon$, with $\epsilon = (\epsilon_1, ..., \epsilon_n)$.\
\
The true regression model is : $Y = X\hat{\beta}$.\
It is realized only if the output is exactly a linear function of the dependant variables (no residual).

### Question 3

```{r}

y <- rnorm(6000, mean = 0, sd = 1)
beta <- solve(t(X) %*% X) %*% t(X) %*% y
xbeta <- X %*% beta
reg <- lm(y~xbeta)
summary(reg)
```

## Exercise 2

**2**)



**b**)
if we take the formula of both predictors in replacing, we have :
$$\begin{aligned}
Y_{i} = 2 + 4X_{1,i} + \epsilon_{2,i} + \epsilon_{3,i} \\
      =  2 + X_{2,i} + \epsilon_{1,i} + \epsilon_{3,i}
\end{aligned}$$


In theory, we should have for the first predictor $\beta_{0} = 2$ and $\beta_{1} = 4$ then for the second predictor $\beta_{0} = 2$ and $\beta_{2} = 1$. We realize the linear regression in R. 
```{r}
set.seed(3)
n=10
X1=rnorm(n)
X2=3*X1+rnorm(n)
Y=2+X1+X2+rnorm(n)
predictor1 <- lm(Y~X1)
predictor2 <- lm(Y~X2)
b01=predictor1$coefficients[1]
b1=predictor1$coefficients[2]
b02=predictor2$coefficients[1]
b2=predictor2$coefficients[2]
summary(predictor1)
summary(predictor2)
list(beta0=b01,beta1=b1,beta0=b02,beta1=b2)


```


First of all, the two models have a good R-square so that seems to be good models.
But, in the first model, the $\beta_{0}$ is enough far of the theory value and in the second model, it's the $\beta_{2}$.
So, the predictors are not really good that's perhaps due to the sum of the different residuals terms. 


