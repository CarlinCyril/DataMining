---
title: "Practical 1"
author: "Blanco Romain, Carlin Cyril, Dong St√©phane"
date: "02/26/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Exercise 1

### Question a

```{r}
# Define the parameters
n <- 6000
p <- 201
# Set the seed to 0
set.seed(0)
# Create the matrix for the Question 1
X <- matrix(nrow = n, ncol = p)
for (i in 1:n) {
  X[i,] = matrix(rnorm(p, mean = 0, sd = 1), nrow = 1, ncol = p)
}
# Store it into a data frame
D <- data.frame(variables=X)
```

### Question b

We use the matrix X defined in the previous question.\
Let $y$ be the 1st variable : $$ y = \begin{pmatrix} x_{1,1} \\
\vdots \\
x_{n,1}\end{pmatrix}$$\
We define, 
$$ MatX = \begin{pmatrix} 1 & x_{1,2} & \cdots & x_{1,p} \\ 
1 & x_{2,2} & \cdots & x_{2,p} \\
\vdots & \vdots & \ddots & \vdots \\
1 & x_{n,2} & \cdots & x_{n,p} \end{pmatrix}$$\

We obtain : $\hat\beta = (MatX^{T}\ MatX)^{-1}\ MatX^{T}\ y$.\
The linear model using the last 200 variables to predict the first one is : $y = X\hat\beta + \epsilon$, with $\epsilon = (\epsilon_1, ..., \epsilon_n)$.\
\
The true regression model is : $y = f(X) + \epsilon$.\
The difference is that for the true regression model, f can be a non-linear function depending on the data.\
The linear regression model can be a possibility for the true regression model.

### Question c

We use the following code to get the coefficients ($\hat\beta$) : \
```{r}
# Define the parameters
n <- 6000
p <- 201
# Set the seed to 0
set.seed(0)
# Create the matrix for the Question 1
X <- matrix(nrow = n, ncol = p)
for (i in 1:n) {
  X[i,] = matrix(rnorm(p, mean = 0, sd = 1), nrow = 1, ncol = p)
}
# Store it into a data frame
D <- data.frame(variables=X)
# Create the design matrix
MatX <- cbind(1, X[,2:p])
y <- D$variables.1
# Simulate the linear regression model, get the betas and the epsilons
fit <- lm(y ~ MatX)
beta <- summary(fit)$coef[,"Estimate"]
epsilon <- fit$residuals
```
The linear model is thus : $$ y = MatX \times \hat\beta + \hat\epsilon$$ with $\hat\beta$ and $\hat\epsilon$ calculated above (called "beta" and "epsilon").\
\
Then, we compute the number of coefficients assessed as significantly non-zero at level 5% with the following code :\
```{r, echo=FALSE}
# Compute the number of coefficients assessed as significantly non-zero at level 5%
level <- 0.05
P_values <- summary(fit)$coef[,"Pr(>|t|)"]
counter <- 0
for (i in P_values) {
  if (i < level)
    counter = counter + 1
}
sprintf("Number of coefficients assessed as significantly non-zero at level %d %% : %d out of %d", level * 100, counter, p)
```
We observe that only around 5.5% (11 out of 201) of the coefficients can be considered as non-zero at level 5%.\
That result is not very surprising because all of the variables are independant. The first variable should not be dependant on (almost) any of the other 200 variables. There are still some variables (close to 5% of them) that are assessed as significantly non-zero at level 5% but that is quite normal due to the 5% tolerance level that we have set.

## Exercise 2

### Question a
```{r, echo=FALSE}
# Set the seed to 3 and define parameter n
set.seed(3)
n <- 1000
# Create data
X1 <- rnorm(n, mean = 0, sd = 1)
X2 <- 3 * X1 + rnorm(n, mean = 0, sd = 1)
Y <- 2 + X1 + X2 + rnorm(n, mean = 0, sd = 1)
# plot the cloud of points
plot(X1, X2, main = "Cloud of points (X1, X2)")
```
\
The cloud of points $(X_{1,i}, X_{2,i})_i$ is very close to a straight line because $X_2$ is defined as a linear  function of $X_1$. Also, as $\epsilon$ has a normal distribution between 0 and 1, it explains the width of 1.

### Question b
```{r, echo=FALSE}
# Set the seed to 3 and define parameter n
set.seed(3)
n <- 10
# Create data
X1 <- rnorm(n, mean = 0, sd = 1)
X2 <- 3 * X1 + rnorm(n, mean = 0, sd = 1)
Y <- 2 + X1 + X2 + rnorm(n, mean = 0, sd = 1)
# Estimate models
fit1 <- lm(Y ~ X1)
fit2 <- lm(Y ~ X2)
beta1 <- summary(fit1)$coef[,"Estimate"]
beta2 <- summary(fit2)$coef[,"Estimate"]
epsilon1 <- fit1$residuals
epsilon2 <- fit2$residuals
summary(fit1)
summary(fit2)
```
\
First of all, the two models have good R-squares so the linear model seems to be a good choice.\
For both models, the two predictors ($\hat\beta_0$ and $\hat\beta_1$ for the first ; $\hat\beta_0$ and $\hat\beta_2$ for the second) can be assessed as non-zero at level 1% (thanks to their p-value). It is thus reasonable to state that, for each model, the two predictors are necessary to describe the model.

### Question c
```{r, echo=FALSE}
# Set the seed to 3 and define parameter n
set.seed(3)
n <- 10
# Create data
X1 <- rnorm(n, mean = 0, sd = 1)
X2 <- 3 * X1 + rnorm(n, mean = 0, sd = 1)
Y <- 2 + X1 + X2 + rnorm(n, mean = 0, sd = 1)
# Estimate model
fit3 <- lm(Y ~ X1 + X2)
beta3 <- summary(fit3)$coef[,"Estimate"]
epsilon3 <- fit3$residuals
summary(fit3)
```
\
By looking at the p-values, we can say that, as opposed to $\hat\beta_1$ and $\hat\beta_2$, $\hat\beta_0$ can be assessed as non-zero at level 5 %.\
That is different from before. Indeed, in the previous question, we tried to explain $Y$ with $X_1$ and $X_2$ separetely and the results were quite different : both $\hat\beta_1$ and $\hat\beta_2$ could be assessed as non-zero at level 5%.\
Also, their estimates were almost two times higher ($\hat\beta_1 = 4.0861$ and $\hat\beta_2 = 1.4239$) than with this model where we try to explain $Y$ with $X_1$ and $X_2$ at the same time ($\hat\beta_1 = 2.0628$ and $\hat\beta_2 = 0.7336$).\
Only the estimates of $\hat\beta_0$ are consistent (1.3733 and 1.7662 separately ; 1.5812 together).\
\
This could be explained by the fact that the two predictor variables $X_1$ and $X_2$ are correlated ($X_2$ is indeed defined using $X_1$).\


### Question d

```{r, echo=FALSE}
ordo1 <- c()
ordo2 <- c()
for (i in 1:100) {
  X1 <- rnorm(n, mean = 0, sd = 1)
  X2 <- 3 * X1 + rnorm(n, mean = 0, sd = 1)
  Y <- 2 + X1 + X2 + rnorm(n, mean = 0, sd = 1)
  fit4 <- lm(Y ~ X1 + X2)
  beta4 <- summary(fit4)$coef[,"Estimate"]
  ordo1 <- cbind(ordo1, beta4[2])
  ordo2 <- cbind(ordo2, beta4[3])
}
par(mfrow=c(1,2))
quant <- quantile(ordo1, probs = c(0.025, 0.975))
plot(density(ordo1), main = "PDF of beta1 given X1", xlab = "X1")
abline(v=quant[1], col="red")
abline(v=quant[2], col="red")
quant <- quantile(ordo2, probs = c(0.025, 0.975))
plot(density(ordo2), main = "PDF of beta2 given X2", xlab = "X2")
abline(v=quant[1], col="red")
abline(v=quant[2], col="red")
```
\
We observe that the two pdf are close to the pdf of a normal distribution $N(0,1)$.

### Question e

```{r, echo=FALSE}
set.seed(3)
n=10
X1=rnorm(n)
X2=3*X1+rnorm(n)
Y=2+X1+X2+rnorm(n)
m1 = mean(X1)
m2 = mean(X2)
m = c(m1,m2)
C <- matrix(nrow = 2, ncol = 2)
C[1,1]=var(X1)
C[2,2]=var(X2)
C[1,2]=cov(X1,X2)
C[2,1]=C[1,2]

ellipses = function(m, CV, probs)
{
  # Compute and plot an ellipse region for bivariate Gaussians, i.e., some ellipse that 
  # has probability probs to contain a 2D Gaussian vector with given parameters.
  # Inputs: 
  #   - m: means
  #   - CV: covariance matrix
  #   - probs: probabilities
  # Source : https://waterprogramming.wordpress.com/2016/11/07/plotting-probability-ellipses-for-bivariate-normal-distributions/
  
  # Coordinates of mean
  b1 = m[1]
  b2 = m[2]
  
  eg = eigen(CV)
  Evec = eg$vectors
  Eval = eg$values
  
  theta = seq(0,2*pi,0.01) # angles used for plotting ellipses
  
  vec.norm = function(v) { sqrt(t(v) %*% v)}
  # compute angle for rotation of ellipse
  # rotation angle will be angle between x axis and first eigenvector
  x.vec = c(1, 0) # vector along x-axis
  cosrotation = t(x.vec) %*% Evec[,1]/(vec.norm(x.vec)*vec.norm(Evec[,1]))
  rotation = pi/2-acos(cosrotation) # rotation angle
  #create a rotation matrix
  R  = matrix(c(sin(rotation), cos(rotation), -cos(rotation), sin(rotation)), 
              nrow=2, ncol=2, byrow = TRUE)
  
  # Create chi squared vector
  chisq = qchisq(probs,2) # percentiles of chi^2 dist df=2
  
  # size ellipses for each quantile
  xRadius = rep(0, length(chisq))
  yRadius = rep(0, length(chisq))
  x = list()
  y = list()
  x.plot = list()
  y.plot = list()
  rotated_Coords = list()
  for (i in 1:length(chisq)) {
    # calculate the radius of the ellipse
    xRadius[i]=(chisq[i]*Eval[1])^.5; # primary axis
    yRadius[i]=(chisq[i]*Eval[2])^.5; # secondary axis
    # lines for plotting ellipse
    x[[i]] = xRadius[i]* cos(theta);
    y[[i]] = yRadius[i] * sin(theta);
    # rotate ellipse
    rotated_Coords[[i]] = R %*% matrix(c(x[[i]], y[[i]]), nrow=2, byrow=TRUE)
    # center ellipse
    x.plot[[i]] = t(rotated_Coords[[i]][1,]) + b1
    y.plot[[i]] = t(rotated_Coords[[i]][2,]) + b2}
  
  xlim = range(x.plot[[i]])
  ylim = range(y.plot[[i]])
  plot(b1,b2, xlab = "X1", ylab = "X2", xlim = xlim, ylim = ylim, cex=1)
  abline(h=0)
  abline(v=0)
  # Plot contours
  for (j in 1:length(chisq)) {
    points(x.plot[[j]],y.plot[[j]], cex=0.1)}
  
  legend("bottomright", c('Ellipse region', paste(probs)))}



ellipses(m,C,c(0.5,0.9,0.999,1-2.92e-05))

```

The ellipses represent possible values for $\beta_{1}$ and $\beta_{2}$ with the probability given. The point (0,0) doesn't pass through the last ellipse because it represent the p-value and so the hypothesis $H_{0}$ is rejected.



