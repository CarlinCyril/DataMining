---
title: "Lab 2"
author: "Blanco Romain, Carlin Cyril, Dong St√©phane"
date: ''
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## 1 Data :

```{r, warning=FALSE}
NAm2 = read.table("NAm2.txt", header = TRUE)
names=unique(NAm2$Pop)
npop=length(names)
coord=unique(NAm2[,c("Pop","long","lat")]) #coordinates for each pop
colPalette=rep(c("black","red","cyan","orange","brown","blue","pink","purple","darkgreen"),3)
pch=rep(c(16,15,25),each=9)
plot(coord[,c("long","lat")],pch=pch,col=colPalette,asp=1)
# asp allows to have the correct ratio between axis longitude and latitude
# Then the map is not deformed
legend("bottomleft",legend=names,col=colPalette,lty=-1,pch=pch,cex=.75,ncol=2,lwd=2)
library(maps);map("world",add=T)

```

This script displays on map each individual thanks to the latitude and the longitude. First of all, the data table is grouped by population thanks to the function <I> unique() </I> then the latitude and longitude are recuperated and there are the coordinates for each population. Finally, a <I> plot() </I> and the package maps display the result. 


## 2 Regression :


```{r}
sink(file = "Res.txt")
NAm2 = read.table("NAm2.txt", header = TRUE)
NAaux = NAm2[,-c(1:7)]

dataframe = data.frame(NAaux)

regression = lm(long ~ .,data = dataframe)


```


```{r, include=FALSE}
summary(regression)
sink()

```

The summary is visible in the file Res.txt. We can see in this file line like this :

L2.264                   -3.5460         NA      NA       NA


The linear regression model doesn't work because t-values and p-value aren't calculable, there are NA instead of figures in the summary. That's due to the fact that the number of observations (samples $n=494$) is lower than the number of variables (markers $p=5709$ ).




## 3 PCA :

## a) 

The PCA is a method which transforms correlated variables in uncorrelated variables. These new variables are called principal components. That allows to synthesize the information and eliminate the useless information. 

The PCA method is designed to reduce dimensionality of a given sample. We want to reduce the dimensionality because most of the methods we have are statistical and so their efficiency is directly related to the dimensionality. We can reduce dimensionality when we have redundance among our attributes (correlated dimensions), useless attributes or not enough data to fully represents all dimensions. PCA's goal is representing instances with fewer variables and trying to preserve data's structure (that affects class separability) as much as possible at the same time. It does that by operating a feature extraction : this method constructs a new set of dimensions linearly combining the dimensions of the original set. This combination is done such as the created axes have the greatest variances. Afterward, the dataset is projected onto this new set of dimensions and have fewer attributes. Thus, regular methods can be more easily applied.


## b)
```{r}
geneticData <- NAm2[,-c(1:8)]
pcaNAm2 <- prcomp(geneticData)
```
Since the datas are all ones and zeros (same unit), we do not need to use the argument scale of prcomp.


## c)
```{r, echo=FALSE}
# Axes 1 and 2
caxes=c(1,2) 
plot(pcaNAm2$x[,caxes],col="white")
for (i in 1:npop) {
  lines(pcaNAm2$x[which(NAm2[,3]==names[i]),caxes], type="p",col=colPalette[i],pch=pch[i])
}
legend("bottomleft",legend=names,col=colPalette,lty=- 1,pch=pch,cex=.5,ncol=3,lwd=2)

# Axes 5 and 6
caxes=c(5,6) 
plot(pcaNAm2$x[,caxes],col="white")
for (i in 1:npop) {
  lines(pcaNAm2$x[which(NAm2[,3]==names[i]),caxes],
        type="p",col=colPalette[i],pch=pch[i])
}
legend("top",legend=names,col=colPalette,lty=- 1,pch=pch,cex=.75,ncol=3,lwd=2)
```
\
The graph represents the influence that some genetic markers have on the populations.
A lot of populations are very close to each other.  
However, some of them are quite distinguishable, for example :  
Axes 1 and 2 : Ache and Surui  
Axes 5 and 6 : Pima and Karitiana  


## d)
The first 2 principal components capture 3.568 % of the variance ("summary(pcaNAm2)").\
In order to represent genetic markers efficiently using a minimal number of principal components, we need the components to have at least a cumulative proportion of the variance of around 60-70 %.\
With the results that we have, we would choose to keep the first 200 principal components (68.217%).\
Note : this result confirms our observation in question c. Indeed the graph 1 we ploted with the first 2 principal components as axes can barely make 2 populations distinguishable and this is now understandable as their cumulative proportion of the variance is only 3.5%.

## 4 PCR


## a)

```{r}
PCAaxes <- as.data.frame(pcaNAm2$x[,1:250])
LatVector <- NAm2$lat
lmlat <- lm(LatVector~., data = PCAaxes)
LongVector <- NAm2$long
lmlong <- lm(LongVector~.,data=PCAaxes)

plot(lmlong$fitted.values,lmlat$fitted.values,col="white", asp=1, xlab="Longitude", ylab="Latitude")
for (i in 1:npop) {
  lines(lmlong$fitted.values[which(NAm2[,3]==names[i])],lmlat$fitted.values[which(NAm2[,3]==names[i])],type="p",col=colPalette[i],pch=pch[i])
}
legend("bottomleft",legend=names,col=colPalette,lty=- 1,pch=pch,cex=.50,ncol=3,lwd=2)
map("world",add=T)
```

We can immediately note that the precision is lower than at the question 1 (obviously). The markers for a given population are no longer piled up at the same spot but more or less, according to the given population, spread on the map.

## b)

```{r,warning=FALSE,message=FALSE}
library("fields")
u = matrix((lmlong$fitted.values), ncol=1)
u = cbind(u, matrix(lmlat$fitted.values, ncol = 1))
v = matrix(NAm2$long, ncol = 1)
v = cbind(v, matrix(NAm2$lat, ncol = 1))
error <- diag(rdist.earth(u,v))
summary(error)
```
The mean error of our regression is approximatively 400km, which is quite imprecise. Although we reach a precision of 21.95km, we also have an error of 1540km on one point. Perhaps taking more PCA axes would improve the precision.

## 5. PCR and cross-validation
## a)
The danger with having one training set and one test set is that our model has captured the noise from our training set and becomes biased. The obtained model could work on our test set, it would not be reliable for it is likely the model is overfitting. Subsequently, the accuracy of our model would be really poor on a new set of explicative variables. To counter this effect, the cross-validation method split our set in several subset of variables that will be used to create a training set, a validation set and a test set.   
Here we will use the K-fold cross validation method that consists in splitting our original set in K parts and make combinations among them to create different test and training sets. Computing the average error gives us the average coefficients for our model and so the likelihood of overfitting is lowered.

## b) and c)
```{r,warning=FALSE,message=FALSE}
library("caret")
library("fields")
l <- length(NAm2$IndivID)
labels=rep(1:10,each=50, length.out = l)
set=sample(labels,l)

# b steps
plot(1, type="n", xlab="Number of components", ylab="Mean Prediction Error (km)", 
     xlim=c(0, 500), ylim=c(0, 2000))
j = 0
errorPCR = matrix(nrow= length(seq(2,440, by=10)), ncol = l)

for(naxes in seq(2,440, by=10)) {
  pcalong=data.frame(cbind(long=NAm2[,c("long")],pcaNAm2$x[,1:naxes]))
  pcalat=data.frame(cbind(lat=NAm2[,c("lat")],pcaNAm2$x[,1:naxes]))
  j = j+1
  predictCoord = matrix(ncol = 2, nrow = l, 
                         dimnames = list(c(),c("longitude", "latitude")))
  for(i in 1:10) {
    fitlong = lm(long~., data = pcalong, subset = which(set != i))
    fitlat = lm(lat~., data = pcalat, subset = which(set != i))
    predictCoord[which(set==i),"longitude"] = predict(fitlong, pcalong[which(set == i),])
    predictCoord[which(set==i),"latitude"] = predict(fitlat, pcalat[which(set==i),])
  }
  errorPCR[j,]<- diag(rdist.earth(predictCoord,v))
  points(naxes,mean(errorPCR[j,]))
}
```



## d)
We keep the model with 73 components because on the graph above it's the number of components which minimizes the error according to the cross validation. 

```{r}
PCAaxes <- as.data.frame(pcaNAm2$x[,1:73])
LatVector <- NAm2$lat
lmlat <- lm(LatVector~., data = PCAaxes)
LongVector <- NAm2$long
lmlong <- lm(LongVector~.,data=PCAaxes)

plot(lmlong$fitted.values,lmlat$fitted.values,col="white", asp=1, xlab = "Longitude", ylab = "Latitude")
for (i in 1:npop) {
  lines(lmlong$fitted.values[which(NAm2[,3]==names[i])],lmlat$fitted.values[which(NAm2[,3]==names[i])],type="p",col=colPalette[i],pch=pch[i])
}
legend("bottomleft",legend=names,col=colPalette,lty=- 1,pch=pch,cex=.50,ncol=3,lwd=2)
map("world",add=T)
```

```{r}
library("fields")
u = matrix((lmlong$fitted.values), ncol=1)
u = cbind(u, matrix(lmlat$fitted.values, ncol = 1))
v = matrix(NAm2$long, ncol = 1)
v = cbind(v, matrix(NAm2$lat, ncol = 1))
error <- diag(rdist.earth(u,v))
summary(error)
```

There are points in the water and the error is bigger than the training model. However we are more certain the model's predictive ability will not depend on the dataset as we avoid overfitting predictions through the PCR method.

# 6)

The conducted study's goal was to obtain results on the impact of PCA for dimensionality reduction on prediction of American tribes using linear regression. We can summarize our study as follows:

* We previously used PCA for data visualization / analysis by saving only 2 PCs.

* After plotting our data transformation with the first two PCs, we noticed that this 2-D space wasn't sufficient for linear seperation of our classes.

* The conclusion of this analysis was to save more components so that our new dataset still holds at least 80 percent of the explained variance.

* We then computed the prediction error for 2 to 440 PC using cross-validation and chose the best number of Principal Components: 73.

* Finally, we used this model to predict all the dataset (result of cross-validation) and plot the results. We see that our results mostly fall around the initial coordinates, a lot of those predictions are on water.


Our final pipeline to predict the geographical coordinates of populations based on their genetical mutations is obtained by the following procedure:

* Don't standardize the data, all our features are already on the same scale and standardizing the data will deterior the prediction ability of our model.

* Apply PCA to construct a projection matrix from the top (in terms of eigenvalues) 73 eigenvectors.

* Train the model using linear regression.

* Use the model to predict geographical coordinates of new samples.

Our model's efficiency can be characterized using the following metrics:

* Mean prediction accuracy in km: 567.90.

Finally, there are mutliple ways our model could be improved:

* We could force the model to predict coordinated that are on land (by adding a penalty on predictions that land on water), it might force the algorithm to give better predictions.

* Since our populations only have one value to characterize their location, we could see this learning problem as a classification problem and use a different machine learning algorithm to get better results.

* Following the same idea as the first point, if we had a circle of possibiliy instead of a center point, the linear regression would give better results since it wouldn't predict those centers.
